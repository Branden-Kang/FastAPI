{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxFkTAs3oOuI7FsoXwBiGr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://ai.plainenglish.io/introducing-fastmcp-v2-the-pythonic-way-to-build-secure-mcp-servers-and-clients-for-ai-fcc09b84771a)"
      ],
      "metadata": {
        "id": "cjB9-EhhsOfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastmcp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk1zETjbsaJN",
        "outputId": "12cb3c06-7042-45b8-d3f3-c1312163b832"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastmcp\n",
            "  Downloading fastmcp-2.12.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: authlib>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from fastmcp) (1.6.3)\n",
            "Collecting cyclopts>=3.0.0 (from fastmcp)\n",
            "  Downloading cyclopts-3.23.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting exceptiongroup>=1.2.2 (from fastmcp)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from fastmcp) (0.28.1)\n",
            "Requirement already satisfied: mcp<2.0.0,>=1.12.4 in /usr/local/lib/python3.12/dist-packages (from fastmcp) (1.13.1)\n",
            "Collecting openapi-core>=0.19.5 (from fastmcp)\n",
            "  Downloading openapi_core-0.19.5-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting openapi-pydantic>=0.5.1 (from fastmcp)\n",
            "  Downloading openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.12/dist-packages (from pydantic[email]>=2.11.7->fastmcp) (2.11.7)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp) (1.9.0)\n",
            "Requirement already satisfied: python-dotenv>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp) (1.1.1)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from fastmcp) (13.9.4)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib>=1.5.2->fastmcp) (43.0.3)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp) (0.17.0)\n",
            "Collecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=3.0.0->fastmcp)\n",
            "  Downloading rich_rst-1.3.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from exceptiongroup>=1.2.2->fastmcp) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->fastmcp) (0.16.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp) (0.4.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp) (0.47.3)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp) (0.35.0)\n",
            "Collecting isodate (from openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jsonschema-path<0.4.0,>=0.3.1 (from openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading jsonschema_path-0.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp) (10.8.0)\n",
            "Collecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading openapi_schema_validator-0.6.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting openapi-spec-validator<0.8.0,>=0.7.1 (from openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading openapi_spec_validator-0.7.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting parse (from openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting werkzeug<3.1.2 (from openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.4.1)\n",
            "Collecting email-validator>=2.0.0 (from pydantic[email]>=2.11.7->fastmcp)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->fastmcp) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->fastmcp) (2.19.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->fastmcp) (1.3.1)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp) (0.27.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp) (6.0.2)\n",
            "Collecting pathable<0.5.0,>=0.4.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading pathable-0.4.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp) (2.32.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp) (0.1.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp) (0.1.4)\n",
            "Collecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp)\n",
            "  Downloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.12/dist-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp) (0.21.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp<2.0.0,>=1.12.4->fastmcp) (8.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug<3.1.2->openapi-core>=0.19.5->fastmcp) (3.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib>=1.5.2->fastmcp) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib>=1.5.2->fastmcp) (2.22)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp) (2.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from rfc3339-validator->openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp) (1.17.0)\n",
            "Downloading fastmcp-2.12.2-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.0/312.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cyclopts-3.23.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading openapi_core-0.19.5-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading jsonschema_path-0.3.4-py3-none-any.whl (14 kB)\n",
            "Downloading openapi_schema_validator-0.6.3-py3-none-any.whl (8.8 kB)\n",
            "Downloading openapi_spec_validator-0.7.2-py3-none-any.whl (39 kB)\n",
            "Downloading rich_rst-1.3.1-py3-none-any.whl (11 kB)\n",
            "Downloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathable-0.4.4-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: parse, werkzeug, pathable, lazy-object-proxy, isodate, exceptiongroup, dnspython, jsonschema-path, email-validator, rich-rst, openapi-pydantic, openapi-schema-validator, cyclopts, openapi-spec-validator, openapi-core, fastmcp\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "Successfully installed cyclopts-3.23.1 dnspython-2.7.0 email-validator-2.3.0 exceptiongroup-1.3.0 fastmcp-2.12.2 isodate-0.7.2 jsonschema-path-0.3.4 lazy-object-proxy-1.12.0 openapi-core-0.19.5 openapi-pydantic-0.5.1 openapi-schema-validator-0.6.3 openapi-spec-validator-0.7.2 parse-1.20.2 pathable-0.4.4 rich-rst-1.3.1 werkzeug-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server Setup (mcp_server.py)"
      ],
      "metadata": {
        "id": "OvIvd2tNsVLn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "Nzym2HQpsLSy",
        "outputId": "668cc388-9669-47ce-85ce-e86b8e8dc6bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Already running asyncio in this thread",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4285229551.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Start server with STDIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Start server with HTTP on port 8000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# mcp.run(transport=\"http\", host=\"127.0.0.1\", port=8000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fastmcp/server/server.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, transport, show_banner, **transport_kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         anyio.run(\n\u001b[0m\u001b[1;32m    379\u001b[0m             partial(\n\u001b[1;32m    380\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_async\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anyio/_core/_eventloop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(func, backend, backend_options, *args)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Already running {asynclib_name} in this thread\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Already running asyncio in this thread"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "from fastmcp import FastMCP\n",
        "from fastmcp.prompts.prompt import PromptMessage, TextContent\n",
        "\n",
        "# Create a basic server instance\n",
        "mcp = FastMCP(name=\"MyMCPServer\")\n",
        "\n",
        "# Tool\n",
        "@mcp.tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers\"\"\"\n",
        "    return a + b\n",
        "\n",
        "# Resource\n",
        "@mcp.resource(\"resource://greeting\")\n",
        "def get_greeting() -> str:\n",
        "    \"\"\"Provides a simple greeting message.\"\"\"\n",
        "    return \"Hello from FastMCP Resources!\"\n",
        "\n",
        "# Resource Template\n",
        "@mcp.resource(\"data://{name}/greeting\")\n",
        "def get_greeting_by_name(name: str) -> str:\n",
        "    \"\"\"Provides a greeting message for a specific name.\"\"\"\n",
        "    return f\"Hello {name} from FastMCP Resources!\"\n",
        "\n",
        "# Basic prompt returning a string (converted to user message automatically)\n",
        "@mcp.prompt\n",
        "def ask_about_topic(topic: str) -> str:\n",
        "    \"\"\"Generates a user message asking for an explanation of a topic.\"\"\"\n",
        "    return f\"Can you please explain the concept of '{topic}'?\"\n",
        "\n",
        "# Prompt returning a specific message type\n",
        "@mcp.prompt\n",
        "def generate_code_request(language: str, task_description: str) -> PromptMessage:\n",
        "    \"\"\"Generates a user message requesting code generation.\"\"\"\n",
        "    content = f\"Write a {language} function that performs the following task: {task_description}\"\n",
        "    return PromptMessage(role=\"user\", content=TextContent(type=\"text\", text=content))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start server with STDIO\n",
        "    mcp.run()\n",
        "    # Start server with HTTP on port 8000\n",
        "    # mcp.run(transport=\"http\", host=\"127.0.0.1\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client Interaction (mcp_client.py)"
      ],
      "metadata": {
        "id": "_1VoSRX9sXIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastmcp import Client\n",
        "\n",
        "# connect Local server (STDIO)\n",
        "client = Client(\"mcp_server.py\")\n",
        "\n",
        "# connect HTTP server\n",
        "# client = Client(\"http://127.0.0.1:8000/mcp\")\n",
        "\n",
        "async def call_mcp():\n",
        "    async with client:\n",
        "        # Basic server interaction\n",
        "        await client.ping()\n",
        "\n",
        "        # List available operations\n",
        "        tools = await client.list_tools()\n",
        "        resources = await client.list_resources()\n",
        "        resource_templates= await client.list_resource_templates()\n",
        "        prompts = await client.list_prompts()\n",
        "\n",
        "        print(\"\\n===============================Tools===============================\\n\")\n",
        "        print(\"\\n\".join([tool.name for tool in tools]))\n",
        "        print(\"\\n===============================Resources===============================\\n\")\n",
        "        print(\"\\n\".join([resource.name for resource in resources]))\n",
        "        print(\"\\n===============================Resource Templates===============================\\n\")\n",
        "        print(\"\\n\".join([resource_template.name for resource_template in resource_templates]))\n",
        "        print(\"\\n===============================Prompts===============================\\n\")\n",
        "        print(\"\\n\".join([prompt.name for prompt in prompts]))\n",
        "\n",
        "        # Call tools\n",
        "        result = await client.call_tool(\"add\", {\"a\": 1, \"b\": 2})\n",
        "        print(\"\\n===============================Tool Result===============================\\n\")\n",
        "        print(result.content[0].text)\n",
        "\n",
        "        # Call resources\n",
        "        result = await client.read_resource(\"resource://greeting\")\n",
        "        print(\"\\n===============================Resource Result===============================\\n\")\n",
        "        print(result[0].text)\n",
        "\n",
        "        # Call prompts\n",
        "        messages = await client.get_prompt(\"ask_about_topic\", {\"topic\": \"AI\"})\n",
        "        print(\"\\n===============================Prompt Result===============================\\n\")\n",
        "        print(messages.messages[0].content.text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(call_mcp())"
      ],
      "metadata": {
        "id": "ZkljhqVZsWt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Server Setup (advanced_mcp_features_server.py)"
      ],
      "metadata": {
        "id": "N_--STRsse8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastmcp import FastMCP, Context\n",
        "from fastmcp.server.dependencies import get_context\n",
        "from dataclasses import dataclass\n",
        "import asyncio\n",
        "\n",
        "# Create a basic server instance\n",
        "mcp = FastMCP(name=\"MyAdvancedMCPServer\")\n",
        "\n",
        "\n",
        "#-----------------------Context-------------------------------\n",
        "@mcp.tool\n",
        "async def process_file(file_uri: str, ctx: Context) -> str:\n",
        "    \"\"\"Processes a file, using context for logging and resource access.\"\"\"\n",
        "    # Context is available as the ctx parameter\n",
        "    return \"Processed file\"\n",
        "\n",
        "# Utility function that needs context but doesn't receive it as a parameter\n",
        "async def process_data(data: list[float]) -> dict:\n",
        "    # Get the active context - only works when called within a request\n",
        "    ctx = get_context()\n",
        "    await ctx.info(f\"Processing {len(data)} data points\")\n",
        "\n",
        "@mcp.tool\n",
        "async def analyze_dataset(dataset_name: str) -> dict:\n",
        "    # Call utility function that uses context internally\n",
        "\n",
        "    # load data from file\n",
        "    # data = load_data(dataset_name)\n",
        "\n",
        "    data = [1,2,3,4,5,6,7,8,9,10]\n",
        "    await process_data(data)\n",
        "\n",
        "    return {\"processed\": len(data), \"results\": data}\n",
        "\n",
        "#-----------------------Elicitation---------------------------\n",
        "@dataclass\n",
        "class UserInfo:\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "@mcp.tool\n",
        "async def collect_user_info(ctx: Context) -> str:\n",
        "    \"\"\"Collect user information through interactive prompts.\"\"\"\n",
        "    result = await ctx.elicit(\n",
        "        message=\"Please provide your information\",\n",
        "        response_type=UserInfo\n",
        "    )\n",
        "\n",
        "    if result.action == \"accept\":\n",
        "        user = result.data\n",
        "        return f\"Hello {user.name}, you are {user.age} years old\"\n",
        "    elif result.action == \"decline\":\n",
        "        return \"Information not provided\"\n",
        "    else:  # cancel\n",
        "        return \"Operation cancelled\"\n",
        "\n",
        "#----------------------Progress Reporting-------------------------------\n",
        "@mcp.tool\n",
        "async def process_items(items: list[str], ctx: Context) -> dict:\n",
        "    \"\"\"Process a list of items with progress updates.\"\"\"\n",
        "    total = len(items)\n",
        "    results = []\n",
        "\n",
        "    for i, item in enumerate(items):\n",
        "        # Report progress as we process each item [total is optional]\n",
        "        await ctx.report_progress(progress=i, total=total)\n",
        "\n",
        "        # Simulate processing time\n",
        "        await asyncio.sleep(1)\n",
        "        results.append(item.upper())\n",
        "\n",
        "    # Report 100% completion\n",
        "    await ctx.report_progress(progress=total, total=total)\n",
        "\n",
        "    return {\"processed\": len(results), \"results\": results}\n",
        "\n",
        "#----------------------LLM Sampling---------------------------\n",
        "\n",
        "@mcp.tool\n",
        "async def analyze_sentiment(text: str, ctx: Context) -> dict:\n",
        "    \"\"\"Analyze the sentiment of text using the client's LLM.\"\"\"\n",
        "    prompt = f\"\"\"Analyze the sentiment of the following text as positive, negative, or neutral.\n",
        "    Just output a single word - 'positive', 'negative', or 'neutral'.\n",
        "\n",
        "    Text to analyze: {text}\"\"\"\n",
        "\n",
        "    # Request LLM analysis\n",
        "    response = await ctx.sample(prompt)\n",
        "\n",
        "    # from fastmcp.client.sampling import SamplingMessage\n",
        "    # messages = [\n",
        "    #     SamplingMessage(role=\"user\", content=f\"I have this data: {context_data}\"),\n",
        "    #     SamplingMessage(role=\"assistant\", content=\"I can see your data. What would you like me to analyze?\"),\n",
        "    #     SamplingMessage(role=\"user\", content=user_query)\n",
        "    # ]\n",
        "\n",
        "    #response = await ctx.sample(\n",
        "    #     messages=messages,\n",
        "    #     system_prompt=\"You are an expert Python programmer. Provide concise, working code examples without explanations.\",\n",
        "    #     model_preferences=\"claude-3-sonnet\",  # Prefer a specific model\n",
        "    #     include_context=\"thisServer\",  # Use the server's context\n",
        "    #     temperature=0.7,\n",
        "    #     max_tokens=300\n",
        "    # )\n",
        "\n",
        "    # Process the LLM's response\n",
        "    sentiment = response.text.strip().lower()\n",
        "\n",
        "    # Map to standard sentiment values\n",
        "    if \"positive\" in sentiment:\n",
        "        sentiment = \"positive\"\n",
        "    elif \"negative\" in sentiment:\n",
        "        sentiment = \"negative\"\n",
        "    else:\n",
        "        sentiment = \"neutral\"\n",
        "\n",
        "    return {\"text\": text, \"sentiment\": sentiment}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Start server with HTTP on port 8000\n",
        "    mcp.run(transport=\"http\", host=\"127.0.0.1\", port=8000)"
      ],
      "metadata": {
        "id": "0sLZqOpVshic"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Server Clients and Handlers"
      ],
      "metadata": {
        "id": "4IPW0WUpskuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiserver Client (multiserver_mcp_client.py):"
      ],
      "metadata": {
        "id": "fjr3gJz_siwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastmcp import Client\n",
        "from fastmcp.client.elicitation import ElicitResult\n",
        "from fastmcp.client.logging import LogMessage\n",
        "from fastmcp.client.sampling import (\n",
        "    SamplingMessage,\n",
        "    SamplingParams,\n",
        "    RequestContext,\n",
        ")\n",
        "\n",
        "\n",
        "# Local server (STDIO)\n",
        "# client = Client(\"advanced_mcp_features_server.py\")\n",
        "\n",
        "# HTTP server\n",
        "# client = Client(\"http://127.0.0.1:8000/mcp\")\n",
        "\n",
        "# JSON config (multiple servers)\n",
        "# Tools are namespaced by server name eg: server_name_tool_name [my_server_add]\n",
        "# Resources are namespaced by server name eg: resource://server_name/resource_name [resource://my_server/greeting]\n",
        "config = {\n",
        "    \"mcpServers\": {\n",
        "        \"my_advanced_server\": {\n",
        "            \"url\": \"http://127.0.0.1:8000/mcp\",\n",
        "            \"transport\": \"http\"\n",
        "        },\n",
        "        \"my_server\": {\n",
        "            \"command\": \"python\",\n",
        "            \"args\": [\"./mcp_server.py\"],\n",
        "            \"env\": {\"LOG_LEVEL\": \"INFO\"}\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "async def elicitation_handler(message: str, response_type: type, params, context):\n",
        "    # Present the message to the user and collect input\n",
        "    print(message)\n",
        "    name = input(\"Enter name: \")\n",
        "    age = input(\"Enter age: \")\n",
        "\n",
        "    if name == \"\" or age == \"\":\n",
        "        return ElicitResult(action=\"decline\")\n",
        "\n",
        "    # Create response using the provided dataclass type\n",
        "    # FastMCP converted the JSON schema to this Python type for you\n",
        "    response_data = response_type(name=name, age=age)\n",
        "\n",
        "    # You can return data directly - FastMCP will implicitly accept the elicitation\n",
        "    return response_data\n",
        "\n",
        "    # Or explicitly return an ElicitResult for more control\n",
        "    # return ElicitResult(action=\"accept\", content=response_data)\n",
        "\n",
        "\n",
        "async def log_handler(message: LogMessage):\n",
        "    \"\"\"\n",
        "    Handles incoming logs from the MCP server and forwards them\n",
        "    to the standard Python logging system.\n",
        "    \"\"\"\n",
        "    msg = message.data.get('msg')\n",
        "    extra = message.data.get('extra')\n",
        "    level = message.level\n",
        "\n",
        "    print(\"======Log Data from handler======\")\n",
        "    print(msg)\n",
        "    print(extra)\n",
        "    print(level)\n",
        "    print(\"=================================\")\n",
        "\n",
        "    # Convert the MCP log level to a Python log level\n",
        "    # level = LOGGING_LEVEL_MAP.get(message.level.upper(), logging.INFO)\n",
        "\n",
        "    # Log the message using the standard logging library\n",
        "    # logger.log(level, msg, extra=extra)\n",
        "\n",
        "async def progress_handler(\n",
        "    progress: float,\n",
        "    total: float | None,\n",
        "    message: str | None\n",
        ") -> None:\n",
        "    if total is not None:\n",
        "        percentage = (progress / total) * 100\n",
        "        print(f\"Progress: {percentage:.1f}% - {message or ''}\")\n",
        "    else:\n",
        "        print(f\"Progress: {progress} - {message or ''}\")\n",
        "\n",
        "async def sampling_handler(\n",
        "    messages: list[SamplingMessage],\n",
        "    params: SamplingParams,\n",
        "    context: RequestContext\n",
        ") -> str:\n",
        "    print(\"======Sampling Data from handler======\")\n",
        "    print(messages[0].content.text)\n",
        "    print(\"=================================\")\n",
        "    # Your LLM integration logic here\n",
        "    # Extract text from messages and generate a response\n",
        "    return \"neutral\"\n",
        "\n",
        "client = Client(\n",
        "    config,\n",
        "    elicitation_handler=elicitation_handler,\n",
        "    log_handler=log_handler,\n",
        "    progress_handler=progress_handler,\n",
        "    sampling_handler=sampling_handler\n",
        "    )\n",
        "\n",
        "async def call_mcp():\n",
        "    async with client:\n",
        "        # Basic server interaction\n",
        "        await client.ping()\n",
        "\n",
        "        # List available operations\n",
        "        tools = await client.list_tools()\n",
        "        resources = await client.list_resources()\n",
        "        resource_templates= await client.list_resource_templates()\n",
        "        prompts = await client.list_prompts()\n",
        "\n",
        "        print(\"\\n===============================Tools===============================\\n\")\n",
        "        print(\"\\n\".join([tool.name for tool in tools]))\n",
        "        print(\"\\n===============================Resources===============================\\n\")\n",
        "        print(\"\\n\".join([resource.name for resource in resources]))\n",
        "        print(\"\\n===============================Resource Templates===============================\\n\")\n",
        "        print(\"\\n\".join([resource_template.name for resource_template in resource_templates]))\n",
        "        print(\"\\n===============================Prompts===============================\\n\")\n",
        "        print(\"\\n\".join([prompt.name for prompt in prompts]))\n",
        "\n",
        "        # Call tools\n",
        "        result = await client.call_tool(\"my_server_add\", {\"a\": 1, \"b\": 2})\n",
        "        print(\"\\n===============================Tool Result===============================\\n\")\n",
        "        print(result.content[0].text)\n",
        "\n",
        "        # Call resources\n",
        "        result = await client.read_resource(\"resource://my_server/greeting\")\n",
        "        print(\"\\n===============================Resource Result===============================\\n\")\n",
        "        print(result[0].text)\n",
        "\n",
        "        # Call prompts\n",
        "        messages = await client.get_prompt(\"my_server_ask_about_topic\", {\"topic\": \"AI\"})\n",
        "        print(\"\\n===============================Prompt Result===============================\\n\")\n",
        "        print(messages.messages[0].content.text)\n",
        "\n",
        "        # Call advanced server tools with context\n",
        "        result = await client.call_tool(\"my_advanced_server_process_file\", {\"file_uri\": \"file://test.txt\"})\n",
        "        print(\"\\n===============================Advanced Server Tool with context Result===============================\\n\")\n",
        "        print(result.content[0].text)\n",
        "\n",
        "        # Call advanced server tools with elicitation\n",
        "        print(\"\\n===============================Advanced Server Tool with elicitation Result===============================\\n\")\n",
        "        result = await client.call_tool(\"my_advanced_server_collect_user_info\")\n",
        "        print(result.content[0].text)\n",
        "\n",
        "        # Call advanced server tools with logging\n",
        "        print(\"\\n===============================Advanced Server Tool with logging Result===============================\\n\")\n",
        "        result = await client.call_tool(\"my_advanced_server_analyze_dataset\", {\"dataset_name\": \"test.txt\"})\n",
        "        print(result.content[0].text)\n",
        "\n",
        "        # Call advanced server tools with progress reporting\n",
        "        print(\"\\n===============================Advanced Server Tool with progress reporting Result===============================\\n\")\n",
        "        result = await client.call_tool(\"my_advanced_server_process_items\", {\"items\": [\"item1\", \"item2\", \"item3\"]})\n",
        "        print(result.content[0].text)\n",
        "\n",
        "        #Call advanced server tools with LLM sampling\n",
        "        print(\"\\n===============================Advanced Server Tool with LLM sampling Result===============================\\n\")\n",
        "        result = await client.call_tool(\"my_advanced_server_analyze_sentiment\", {\"text\": \"AI is the future of technology\"})\n",
        "        print(result.content[0].text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(call_mcp())"
      ],
      "metadata": {
        "id": "lCaQ26fqshwz"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}